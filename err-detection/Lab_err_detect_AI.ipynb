{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75f9980b",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9170fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-19T02:12:57.857742Z",
     "start_time": "2023-09-19T02:12:51.394779Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "# Data manipulation and numeric libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import scipy.ndimage as ndimage\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Machine learning and optimization libraries\n",
    "import joblib\n",
    "from joblib import dump, load\n",
    "import optuna\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, StratifiedKFold, GroupShuffleSplit, cross_val_score,\n",
    "    RandomizedSearchCV, KFold\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score, accuracy_score, confusion_matrix, precision_score,\n",
    "    recall_score, f1_score, roc_auc_score, precision_recall_curve, auc, \n",
    "    make_scorer, average_precision_score, precision_recall_fscore_support, \n",
    "    roc_curve, ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.utils import resample, class_weight\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, MaxAbsScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, optimizers, backend as K\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import (\n",
    "    GlobalAveragePooling1D, MaxPool1D, AvgPool1D, LeakyReLU, TimeDistributed,\n",
    "    GlobalMaxPooling2D, Bidirectional, ReLU, Reshape, Activation, LSTM,\n",
    "    MaxPool2D, Conv2D, MaxPooling1D, BatchNormalization, Dense, Dropout,\n",
    "    Flatten, Conv1D, GlobalMaxPooling1D, GlobalAveragePooling2D, AvgPool2D,\n",
    "    SimpleRNN\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, Nadam, SGD, Adamax\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "# XGBoost library\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# File and plotting libraries\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.io import savemat\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec29b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_score(model, x, y, print_results=True, deep_model=False, batch_size= 256):\n",
    "    \"\"\"\n",
    "    Predicts the score for the given model and data.\n",
    "    \n",
    "    Args:\n",
    "    - model: Trained model.\n",
    "    - x: Input data.\n",
    "    - y: True labels.\n",
    "    - print_results: Whether to print the results.\n",
    "    - deep_model: Indicates the type of model (True for deep learning models, False for others, -1 for special case).\n",
    "    \n",
    "    Returns:\n",
    "    - probs: Predicted probabilities.\n",
    "    - metrics: List of [tp, tn, fp, fn].\n",
    "    - scores: List of [auc, auprc, acc, sens, sp, ppv, npv].\n",
    "    \"\"\"\n",
    "    \n",
    "    # Predict probabilities based on the model type\n",
    "    if deep_model == True:\n",
    "        probs = model.predict(x, verbose=0, batch_size = batch_size)[:, 0]\n",
    "    elif deep_model == -1:\n",
    "        reconstruction_error = np.square(x[:, 0] - model.predict_proba(x)[:, 1])\n",
    "        probs = 1 - stats.norm.cdf(reconstruction_error)\n",
    "    else:\n",
    "        probs = model.predict_proba(x)[:, 1]\n",
    "    \n",
    "    # Convert probabilities to binary predictions\n",
    "    preds = (probs > 0.5).astype('int32')\n",
    "    \n",
    "    # Compute confusion matrix values\n",
    "    tp = np.sum((preds == 1) & (y == 1))\n",
    "    tn = np.sum((preds == 0) & (y == 0))\n",
    "    fp = np.sum((preds == 1) & (y == 0))\n",
    "    fn = np.sum((preds == 0) & (y == 1))\n",
    "    \n",
    "    # Compute metrics\n",
    "    auc = 1 if np.unique(y).size == 1 else roc_auc_score(y, probs)\n",
    "    auprc = auprc_scoring(y, probs)\n",
    "    npv = tn / (tn + fn)\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    sp = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    sens = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    acc = (sp + sens) / 2\n",
    "    \n",
    "    # Print results if required\n",
    "    if print_results:\n",
    "        print(f'AUC: {auc:.4f}, AUPRC: {auprc:.4f}, Accuracy: {acc:.4f}, Sensitivity: {sens:.4f}, Specificity: {sp:.4f}, NPV: {npv:.4f}, PPV: {ppv:.4f}')\n",
    "    \n",
    "    return probs, [tp, tn, fp, fn], [auc, auprc, acc, sens, sp, ppv, npv]\n",
    "\n",
    "def auprc_scoring(y_true, y_pred_proba):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba)\n",
    "    return auc(recall, precision)\n",
    "\n",
    "class data_sacler: # just for implement\n",
    "    def __init__(self, prescale=False, scaler_opts='RobustScaler', datascale =False,scaleset=False):\n",
    "        self.prescale = prescale\n",
    "        self.scaler_opts = scaler_opts\n",
    "        self.scaleset = scaleset\n",
    "        self.datascale = datascale\n",
    "        self.scaler1 = None\n",
    "        self.scaler2 = None\n",
    "        \n",
    "    def scaler_set(self, scaler1, scaler2):\n",
    "        self.scaler1 = scaler1\n",
    "        self.scaler2 = scaler2\n",
    "    \n",
    "    def transform(self, x_data):\n",
    "        if self.prescale == 'log': \n",
    "            x_data[:,[0,1]] = np.log(x_data[:,[0,1]]) \n",
    "        x_data_tmp = x_data[:,:2].shape\n",
    "        if self.datascale:            \n",
    "            x_data[:,:2] = self.scaler1.transform(x_data[:,:2].reshape(-1,1)).reshape(x_data_tmp)\n",
    "        if self.scaleset:\n",
    "             x_data[:,-1] = self.scaler2.transform(x_data[:,-1].reshape(-1,1)).reshape(-1)\n",
    "        return x_data\n",
    "    \n",
    "    def fit_transform(self, x_data ):\n",
    "        if self.prescale == 'log': \n",
    "            x_data[:,[0,1]] = np.log(x_data[:,[0,1]])\n",
    "        \n",
    "        if self.scaler_opts == 'RobustScaler':\n",
    "            self.scaler1 = RobustScaler()\n",
    "            self.scaler2 = RobustScaler()\n",
    "\n",
    "        elif self.scaler_opts == 'MinMaxScaler':\n",
    "            self.scaler1 = MinMaxScaler()\n",
    "            self.scaler2 = MinMaxScaler()\n",
    "\n",
    "        elif self.scaler_opts == 'StandardScaler':\n",
    "            self.scaler1 = StandardScaler()\n",
    "            self.scaler2 = StandardScaler()\n",
    "            \n",
    "        elif self.scaler_opts == 'MaxAbsScaler':\n",
    "            self.scaler1 = MaxAbsScaler()\n",
    "            self.scaler2 = MaxAbsScaler()\n",
    "        else:\n",
    "            self.scaler1=None\n",
    "            self.scaler2=None\n",
    "            \n",
    "        x_data_tmp = x_data[:,:2].shape\n",
    "        if self.datascale:\n",
    "            x_data[:,:2] = self.scaler1.fit_transform(x_data[:,:2].reshape(-1,1)).reshape(x_data_tmp)\n",
    "        if self.scaleset:\n",
    "            x_data[:,-1] = self.scaler2.fit_transform(x_data[:,-1].reshape(-1,1)).reshape(-1)\n",
    "        \n",
    "        return x_data, self.scaler1, self.scaler2\n",
    "\n",
    "def oversampling(X_train, y_train, smote= False):\n",
    "    \"\"\"\n",
    "    Perform oversampling to address class imbalance in training data.\n",
    "    \n",
    "    Args:\n",
    "    X_train (numpy.array): The training feature data.\n",
    "    y_train (numpy.array): The corresponding training target labels.\n",
    "    smote (bool or int): If True, use SMOTE for oversampling. If -1, no oversampling is performed.\n",
    "                         Any other value triggers random oversampling of the minority class.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.array: The oversampled training feature data.\n",
    "    numpy.array: The corresponding oversampled training target labels.\n",
    "    \"\"\"\n",
    "    if smote:\n",
    "        sm = SMOTE(random_state=42)\n",
    "        X_train_upsampled, y_train_upsampled = sm.fit_resample(X_train,y_train)\n",
    "    elif smote == -1:\n",
    "        X_train_upsampled, y_train_upsampled = X_train, y_train\n",
    "    else:\n",
    "        X_train_1 = X_train[y_train == 1]\n",
    "        y_train_1 = y_train[y_train == 1]\n",
    "\n",
    "        n_samples = (y_train == 0).sum()  \n",
    "        indices = np.random.choice(X_train_1.shape[0], n_samples, replace=True) \n",
    "        X_train_1_upsampled = X_train_1[indices]  \n",
    "        y_train_1_upsampled = y_train_1[indices]  \n",
    "\n",
    "        X_train_upsampled = np.vstack([X_train[y_train == 0], X_train_1_upsampled])\n",
    "        y_train_upsampled = np.concatenate([y_train[y_train == 0], y_train_1_upsampled])\n",
    "    return X_train_upsampled,y_train_upsampled\n",
    "\n",
    "# models\n",
    "def hp_dnn(trial, input_dim):\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 20)  \n",
    "    activ_fcn = trial.suggest_categorical('dense_activation', ['relu', 'elu', 'gelu'])\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam'])\n",
    "    lr = trial.suggest_categorical('lr', [0.001, 0.0005, 0.0001])\n",
    "    input_tensor = tf.keras.layers.Input(shape=input_dim)\n",
    "    x = input_tensor\n",
    "    for i in range(num_layers):\n",
    "        units = trial.suggest_int(name='units_' + str(i), low=32, high=512, step=32)  \n",
    "        x = tf.keras.layers.Dense(units=units, activation=activ_fcn,\n",
    "                                  kernel_initializer=trial.suggest_categorical('kernel_initializer_' + str(i), ['he_normal', 'glorot_uniform', 'lecun_normal']))(x)\n",
    "        batch_norm = trial.suggest_categorical('batch_' + str(i), [True, False])\n",
    "        if batch_norm:\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "        dropout = trial.suggest_categorical('dropout_' + str(i), [True, False])\n",
    "        if dropout:\n",
    "            dropout_rate = trial.suggest_float('dropout_rate_' + str(i), 0.1, 0.5, step=0.1)  # 드롭아웃 비율 조정\n",
    "            x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "        l2_reg = trial.suggest_float('l2_reg_' + str(i), 1e-6, 1e-1, log=True)\n",
    "        x = tf.keras.layers.Dense(units=units, activation=activ_fcn, \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(x)\n",
    "    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs=input_tensor, outputs=x)\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "    # elif optimizer_name == 'Nadam':\n",
    "    #     optimizer = Nadam(learning_rate=lr)\n",
    "    # else:  # RMSprop\n",
    "    #     optimizer = RMSprop(learning_rate=lr)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[tf.keras.metrics.AUC(name='PR', curve='PR'),\n",
    "                           tf.keras.metrics.BinaryAccuracy(name='bacc', threshold=0.5),\n",
    "                           tf.keras.metrics.AUC(curve='ROC')])\n",
    "    return model\n",
    "\n",
    "def hp_xgb(trial):\n",
    "    param = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'booster': trial.suggest_categorical('booster', ['gbtree', 'dart']),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),\n",
    "        'learning_rate': trial.suggest_categorical('lr',[0.001, 0.0005, 0.0001]), #1e-4\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'max_depth': trial.suggest_int('max_depth', 10, 1000),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 40),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 1000, step=10),\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**param)\n",
    "    return model\n",
    "\n",
    "def hp_rf(trial):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 1000, step=100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 100, log=True),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 16, log=True),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 10),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2']),\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
    "        'class_weight': 'balanced',\n",
    "    }    \n",
    "    model = RandomForestClassifier(**param)\n",
    "    return model\n",
    "\n",
    "def hp_lr(trial):\n",
    "    param = {\n",
    "#         'C': trial.suggest_loguniform('C',0.001, 100),  \n",
    "        'penalty': trial.suggest_categorical('penalty', ['none','l2']),\n",
    "#         'solver': trial.suggest_categorical('solver', ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']),\n",
    "        'max_iter': trial.suggest_categorical('max_iter',[100,1000,2500,5000]),\n",
    "        'class_weight': 'balanced',\n",
    "    }\n",
    "\n",
    "# research process function\n",
    "def gen_dbset(x, n_splits=10, scale = True):\n",
    "    \"\"\"\n",
    "    Generates training and testing datasets by splitting the input dataframe.\n",
    "    \n",
    "    Args:\n",
    "    x (pandas.DataFrame): Input data frame that includes at least two columns for features.\n",
    "    n_splits (int): Number of splits to perform on the data. Default is 10.\n",
    "    scale (bool): Whether to apply scaling to the input features. Default is True.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Two lists containing numpy arrays for training and testing datasets respectively.\n",
    "    \"\"\"\n",
    "    \n",
    "    tdf = x.copy()  \n",
    "    tdf = tdf.dropna()  \n",
    "    input_data = tdf.iloc[:,:2].to_numpy()\n",
    "    # clinical_setting = tdf['clinical_setting'] # if exist\n",
    "\n",
    "    #--------------------------------\n",
    "    # Data must be preprocessed in the previous step unconditionally, but added for implementation and execution.\n",
    "    if scale == True:\n",
    "        tscaler = data_sacler('log','RobustScaler',True,False)\n",
    "        scaled_input_data,_,_ = tscaler.fit_transform(input_data.copy())\n",
    "    else:\n",
    "        scaled_input_data = input_data\n",
    "    #--------------------------------\n",
    "\n",
    "    print('=====================')\n",
    "    print('Raw db shape:',scaled_input_data.shape)\n",
    "    \n",
    "    # skf = StratifiedKFold(n_splits = n_splits)\n",
    "    skf = KFold(n_splits=n_splits) \n",
    "    nx_train, nx_test = [],[]\n",
    "    # for nest_idx, (x_train, x_test) in enumerate(skf.split(scaled_input_data,clinical_setting)):\n",
    "    for nest_idx, (x_train, x_test) in enumerate(skf.split(scaled_input_data)):\n",
    "        # print('------------------Nest :',nest_idx)\n",
    "        nx_train.append(scaled_input_data[x_train])\n",
    "        nx_test.append(scaled_input_data[x_test])\n",
    "        # print('train : ',scaled_input_data[x_train].shape,'/','test :',scaled_input_data[x_test].shape)\n",
    "    return nx_train, nx_test\n",
    "\n",
    "def nested_train_model(x_train_all, model_str, n_trials =10, shuffle_ratio = 0.01, verbose_tr =0):\n",
    "    \"\"\"\n",
    "    Trains models specified by `model_str` on multiple datasets using hyperparameter optimization.\n",
    "    The function performs training within a nested structure to optimize hyperparameters for each dataset independently.\n",
    "\n",
    "    Args:\n",
    "    x_train_all (list of numpy.ndarray): List of training datasets.\n",
    "    model_str (str): Specifies the type of model to train. Supported types are 'dnn' (Deep Neural Network),\n",
    "                     'rnn' (Recurrent Neural Network), 'cnn' (Convolutional Neural Network),\n",
    "                     'xgb' (XGBoost), 'rf' (Random Forest), and 'lr' (Logistic Regression).\n",
    "    n_trials (int, optional): Number of trials for hyperparameter optimization (default is 10).\n",
    "    shuffle_ratio (float, optional): Ratio of the data to shuffle to create training variability (default is 0.01).\n",
    "    verbose_tr (int, optional): Verbosity mode for training output, 0 = silent, 1 = progress bar, 2 = one line per epoch (default is 0).\n",
    "\n",
    "    Returns:\n",
    "    tuple: \n",
    "        - list of Optuna study objects containing the results of the optimization.\n",
    "        - list of the best models trained on each dataset.\n",
    "\n",
    "    Each dataset in `x_train_all` is processed to shuffle, oversample, and then split into training and validation subsets.\n",
    "    Model performance is optimized for balanced accuracy using Optuna.\n",
    "    \"\"\"\n",
    "    study_all = []\n",
    "    best_models = []\n",
    "\n",
    "    for nest_idx,x_train in enumerate(x_train_all):\n",
    "        tf.keras.backend.clear_session()       \n",
    "        x_train, y_train = shuffle_data(x_train, shuffle_ratio=shuffle_ratio, random_state = 1111)\n",
    "        x_train, y_train = oversampling(x_train, y_train)\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_train,y_train, test_size=0.25, stratify=y_train)\n",
    "        epoch = 20\n",
    "        study=[]\n",
    "        # class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "        # class_weights = {i: class_weights[i] for i in range(2)}\n",
    "        early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=100, restore_best_weights=True)\n",
    "\n",
    "        def objective(trial):\n",
    "            model = []\n",
    "            if model_str in ('dnn','rnn','cnn'):\n",
    "                input_dim = x_train.shape[1]\n",
    "                if model_str == 'dnn':\n",
    "                    model = hp_dnn(trial, input_dim)\n",
    "                # elif model_str == 'rnn':\n",
    "                #     model = hp_rnn(trial, input_dim)\n",
    "                # elif model_str == 'cnn':\n",
    "                #     model = hp_cnn(trial, input_dim)\n",
    "                model.fit(x_train, y_train, epochs=epoch, validation_data=(x_val, y_val),\n",
    "                                    callbacks=[early_stopping_cb], verbose=verbose_tr, batch_size=256)\n",
    "                preds = (model.predict(x_val)>0.5).astype(int)\n",
    "                \n",
    "            elif model_str == 'xgb':\n",
    "                model = hp_xgb(trial)\n",
    "                model.fit(x_train, y_train,\n",
    "                        eval_set = [(x_train,y_train), (x_val,y_val)],\n",
    "                        eval_metric = 'auc',\n",
    "                        early_stopping_rounds=100, verbose = verbose_tr)\n",
    "                preds = model.predict(x_val)\n",
    "\n",
    "            elif model_str in ('rf','lr'):\n",
    "                if model_str == 'rf':\n",
    "                    model = hp_rf(trial)\n",
    "                if model_str == 'lr':\n",
    "                    model = hp_lr(trial)\n",
    "                model.fit(x_train, y_train)\n",
    "                preds = model.predict(x_val)\n",
    "            return balanced_accuracy_score(y_val, preds)\n",
    "            \n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(objective, n_trials = n_trials, show_progress_bar=True)\n",
    "        study_all.append(study)\n",
    "\n",
    "        best_trial = study.best_trial\n",
    "        if model_str in ('dnn', 'rnn', 'cnn'):\n",
    "            input_dim = x_train.shape[1]\n",
    "            if model_str == 'dnn':\n",
    "                best_model = hp_dnn(best_trial, input_dim)\n",
    "            # elif model_str == 'rnn':\n",
    "            #     best_model = hp_rnn(best_trial, input_dim)\n",
    "            # elif model_str == 'cnn':\n",
    "            #     best_model = hp_cnn(best_trial, input_dim)\n",
    "            best_model.fit(x_train, y_train, epochs=epoch, validation_data=(x_val, y_val),\n",
    "                           callbacks=[early_stopping_cb], verbose=verbose_tr, batch_size=256)\n",
    "        \n",
    "        elif model_str == 'xgb':\n",
    "            best_model = hp_xgb(best_trial)\n",
    "            best_model.fit(x_train, y_train,\n",
    "                           eval_set=[(x_train, y_train), (x_val, y_val)],\n",
    "                           eval_metric='auc',\n",
    "                           early_stopping_rounds=100, verbose=verbose_tr)\n",
    "        \n",
    "        elif model_str in ('rf', 'lr'):\n",
    "            if model_str == 'rf':\n",
    "                best_model = hp_rf(best_trial)\n",
    "            if model_str == 'lr':\n",
    "                best_model = hp_lr(best_trial)\n",
    "            best_model.fit(x_train, y_train)\n",
    "        \n",
    "        best_models.append(best_model)\n",
    "    return study_all, best_models  \n",
    "\n",
    "def shuffle_data(x, shuffle_ratio, random_state=None):\n",
    "    \"\"\"\n",
    "    Shuffles a portion of the data in the input array based on the specified split ratio.\n",
    "    \n",
    "    Args:\n",
    "    x (numpy.array): The input data array to be shuffled.\n",
    "    shuffle_ratio (float): The fraction of the array to shuffle (0 < shuffle_ratio <= 1).\n",
    "    random_state (int, optional): A seed number to make the shuffle deterministic. Default is None.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.array: The shuffled array.\n",
    "    numpy.array: An indicator array where shuffled indices are marked as 1 and others as 0.\n",
    "    \n",
    "    The function copies the input array, shuffles a portion of the first column of the array based on\n",
    "    the split ratio, and returns the shuffled array along with an indicator array that marks which\n",
    "    elements were shuffled.\n",
    "    \"\"\"\n",
    "\n",
    "    # copy\n",
    "    arr = x.copy()\n",
    "    # Determine the indices that should be shuffled\n",
    "    shuffle_idx = np.random.RandomState(random_state).permutation(arr.shape[0])[:int(arr.shape[0] * shuffle_ratio)]\n",
    "    # Shuffle only the first column of the selected indices\n",
    "    arr[shuffle_idx, 0] = np.random.RandomState(random_state).permutation(arr[:, 0][shuffle_idx])\n",
    "    # Create an indicator array that marks shuffled indices\n",
    "    shuffle_indicator = np.zeros(arr.shape[0])\n",
    "    shuffle_indicator[shuffle_idx] = 1\n",
    "    return arr, shuffle_indicator\n",
    "\n",
    "def nested_evaluate_model(x_test_all, optimized_models, model_str, n_iter = 1000, test_shuffle_ratio=0.01):\n",
    "    \"\"\"\n",
    "    Evaluate a list of optimized models on multiple shuffled test datasets to simulate real-world variability \n",
    "    and generate performance metrics.\n",
    "\n",
    "    Args:\n",
    "    x_test_all (list of numpy.ndarray): List of test datasets where each dataset corresponds to a model in `optimized_models`.\n",
    "    optimized_models (list of models): List of trained and optimized models to be evaluated.\n",
    "    model_str (str): Type of the models to determine the prediction approach; options include 'dnn' for deep neural networks and others.\n",
    "    n_iter (int, optional): Number of iterations for generating test sets through permutation, default is 1000.\n",
    "    test_shuffle_ratio (float, optional): Ratio of the test data to shuffle for each iteration to create variability, default is 0.01.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing lists of various performance metrics:\n",
    "        - 'scores': List of overall performance scores for each model.\n",
    "        - 'probabilities': List of predicted probabilities for each test dataset.\n",
    "        - 'tpr': True Positive Rates for ROC curve analysis.\n",
    "        - 'fpr': False Positive Rates for ROC curve analysis.\n",
    "        - 'precision': Precision values for Precision-Recall curve analysis.\n",
    "        - 'recall': Recall values for Precision-Recall curve analysis.\n",
    "\n",
    "    Each metric list contains an element for each model evaluated.\n",
    "    \"\"\"\n",
    "\n",
    "    scores_all, probs_all, tpr_all, fpr_all, prer_all, recall_all = [],[],[],[],[],[]\n",
    "    for nest_idx, x_test in enumerate(x_test_all):\n",
    "        x_test_whole, y_test_whole = [],[]\n",
    "        # error set generation\n",
    "\n",
    "        # Generating test sets with permutation test-based iterative simulation\n",
    "        for i in tqdm(range(n_iter)):\n",
    "            tx_te, ty_te = shuffle_data(x_test,shuffle_ratio=test_shuffle_ratio, random_state=i)\n",
    "            x_test_whole.append(tx_te)\n",
    "            y_test_whole.append(ty_te)\n",
    "        x_test_whole = np.vstack(x_test_whole)\n",
    "        y_test_whole = np.hstack(y_test_whole)\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        best_model = []\n",
    "        best_model = optimized_models[nest_idx]\n",
    "\n",
    "        if model_str == 'dnn':\n",
    "            probs,_,scores = predict_score(best_model,x_test_whole,y_test_whole, print_results=True, deep_model=True)\n",
    "        else:\n",
    "            probs,_,scores = predict_score(best_model,x_test_whole,y_test_whole, print_results=True, deep_model=False)\n",
    "\n",
    "        scores_all.append(scores)\n",
    "        probs_all.append(probs)\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(y_test_whole, probs)\n",
    "        fpr,tpr=interpolate_data(fpr,tpr,inverse = False)\n",
    "        tpr_all.append(tpr)\n",
    "        fpr_all.append(fpr)    \n",
    "\n",
    "        prer,recall,_ = precision_recall_curve(y_test_whole,probs)    \n",
    "        recall,prer=interpolate_data(recall,prer,inverse = True)\n",
    "        prer_all.append(prer)\n",
    "        recall_all.append(recall) \n",
    "\n",
    "    return {\n",
    "        'scores': scores_all,\n",
    "        'probabilities': probs_all,\n",
    "        'tpr': tpr_all,\n",
    "        'fpr': fpr_all,\n",
    "        'precision': prer_all,\n",
    "        'recall': recall_all\n",
    "    } \n",
    "\n",
    "def interpolate_data(x, y, num_points=100, err=0, inverse=False):\n",
    "    \"\"\"\n",
    "    Interpolates or inversely interpolates x and y data points based on the original data and a boolean flag.\n",
    "    Use to match TPR, FPR counts\n",
    "    \n",
    "    Args:\n",
    "    x (numpy.array): Original x values.\n",
    "    y (numpy.array): Original y values.\n",
    "    num_points (int): The number of interpolated data points to generate. Default is 100.\n",
    "    err (int): Error margin to add additional points for edge cases. Default is 0.\n",
    "    inverse (bool): If True, perform inverse interpolation; otherwise, perform normal interpolation. Default is False.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: new_x, new_y containing the interpolated data points.\n",
    "    \n",
    "    Example usage:\n",
    "    # For normal interpolation:\n",
    "    new_x, new_y = interpolate_data(x, y, inverse=False)\n",
    "    # For inverse interpolation:\n",
    "    new_x, new_y = interpolate_data(x, y, inverse=True)\n",
    "    \"\"\"\n",
    "    # Insert start and end values based on the inverse flag\n",
    "    if inverse:\n",
    "        start_end = (1, 0)  # Inverse interpolation\n",
    "    else:\n",
    "        start_end = (0, 1)  # Normal interpolation\n",
    "    \n",
    "    x = np.insert(x, 0, 0)  # Inserting 0 at the beginning of the x array\n",
    "    x = np.append(x, 1)    # Appending 1 at the end of the x array\n",
    "    y = np.insert(y, 0, start_end[0])  # Inserting the start value at the beginning of the y array\n",
    "    y = np.append(y, start_end[1])    # Appending the end value at the end of the y array\n",
    "    \n",
    "    # Create an interpolator object and generate new points\n",
    "    interpolator = interp1d(x, y, kind='linear', fill_value='extrapolate')\n",
    "    new_x = np.linspace(0, 1, num=num_points + err)  # Generating new x values\n",
    "    new_y = interpolator(new_x)  # Generating new y values using the interpolator\n",
    "\n",
    "    return new_x, new_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfea096",
   "metadata": {},
   "source": [
    "# Example of AI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c97d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_all, x_test_all, study_all = [],[],[]\n",
    "\n",
    "# error simulation : shuffle ratio (DO NOT exceed 0.5)\n",
    "train_shuffle_ratio = 0.1\n",
    "test_shuffle_ratio = 0.01\n",
    "\n",
    "# Load data\n",
    "db=pd.read_pickle('../DB/sample_db.pkl')\n",
    "\n",
    "# Generate dataset\n",
    "x_train_all, x_test_all = gen_dbset(db, n_splits=5)\n",
    "\n",
    "# Train and optimize model \n",
    "model_str = 'dnn' # Select model : 'dnn', 'xgb', 'rf', 'lr'\n",
    "study_all, best_models = nested_train_model(x_train_all, model_str, n_trials =1, shuffle_ratio = train_shuffle_ratio)\n",
    "\n",
    "# Evaluation model with permutation test\n",
    "result_all=nested_evaluate_model(x_test_all, best_models, model_str, n_iter = 1000, test_shuffle_ratio=test_shuffle_ratio)\n",
    "avg_scores=np.mean(result_all['scores'], axis=0)\n",
    "metrics_names = [\"AUC\", \"AUPRC\", \"Accuracy\", \"Sensitivity\", \"Specificity\", \"PPV\", \"NPV\"]\n",
    "for name, score in zip(metrics_names, avg_scores):\n",
    "    print(f\"{name}: {score:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec23800",
   "metadata": {},
   "source": [
    "# External validation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f368faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load external DB and Generate exteranl validation dataset\n",
    "external_x_train_all, external_x_test_all = gen_dbset(external_db, n_splits=5)\n",
    "\n",
    "# 1. external validation (use internally developed model)\n",
    "result_all=nested_evaluate_model(external_x_test_all, best_models, model_str, n_iter = 1000, test_shuffle_ratio=shuffle_ratio)\n",
    "\n",
    "# 2. Optimal performance model for external data\n",
    "study_all, best_models = nested_train_model(external_x_train_all, model_str, n_trials =100, shuffle_ratio = shuffle_ratio)\n",
    "result_all=nested_evaluate_model(external_x_test_all, best_models, model_str, n_iter = 1000, test_shuffle_ratio=shuffle_ratio)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.764px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
